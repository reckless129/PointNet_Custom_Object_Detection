{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\wangp\\\\Anaconda3\\\\envs\\\\open3d\\\\python.exe'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import math\n",
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import socket\n",
    "import glob\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import h5py\n",
    "import provider\n",
    "import tf_util\n",
    "from model import *\n",
    "from plyfile import PlyData, PlyElement\n",
    "print(\"success\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "BATCH_SIZE_EVAL = 1\n",
    "NUM_POINT = 4096\n",
    "MAX_EPOCH = 50\n",
    "BASE_LEARNING_RATE = 0.001\n",
    "GPU_INDEX = 0\n",
    "MOMENTUM = 0.9\n",
    "OPTIMIZER = 'adam'\n",
    "DECAY_STEP = 300000\n",
    "DECAY_RATE = 0.5\n",
    "\n",
    "LOG_DIR = 'log'\n",
    "if not os.path.exists(LOG_DIR): os.mkdir(LOG_DIR)\n",
    "os.system('cp model.py %s' % (LOG_DIR)) # bkp of model def\n",
    "#os.system('cp train.py %s' % (LOG_DIR)) # bkp of train procedure\n",
    "LOG_FOUT = open(os.path.join(LOG_DIR, 'log_train.txt'), 'w')\n",
    "# LOG_FOUT.write(str(FLAGS)+'\\n')\n",
    "\n",
    "MAX_NUM_POINT = 4096\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "BN_INIT_DECAY = 0.5\n",
    "BN_DECAY_DECAY_RATE = 0.5\n",
    "#BN_DECAY_DECAY_STEP = float(DECAY_STEP * 2)\n",
    "BN_DECAY_DECAY_STEP = float(DECAY_STEP)\n",
    "BN_DECAY_CLIP = 0.99\n",
    "\n",
    "HOSTNAME = socket.gethostname()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 file \"test.h5\" (mode r)>\n",
      "(4096, 6)\n",
      "(1, 4096)\n"
     ]
    }
   ],
   "source": [
    "# Load ALL data\n",
    "f = h5py.File('data/test.h5')\n",
    "print(f)\n",
    "\n",
    "#Choose a frame to test, (0,60)\n",
    "frame_to_test = 15\n",
    "\n",
    "\n",
    "test_data = np.zeros((4096, 6))\n",
    "test_label = np.ones((1,4096))\n",
    "\n",
    "xmax = 3.0\n",
    "xmin = -3.0\n",
    "\n",
    "data = f['data']\n",
    "label = f['label']\n",
    "test_data[:,0:3] = (data[frame_to_test][:, 0:3]- xmin) / (xmax  - xmin )\n",
    "test_data[:,3:6] = data[frame_to_test][:, 3:6]\n",
    "test_label[:,:] = label[frame_to_test][:]\n",
    "\n",
    "    \n",
    "print(test_data.shape)\n",
    "print(test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_range : 0.33921632170677185 0.5573357939720154\n",
      "y_range : 0.4437101483345032 0.6659368872642517\n",
      "z_range : 0.574175680677096 0.7316666841506958\n",
      "r_range : 0.0 0.003921568859368563\n",
      "g_range : 0.0001845444057835266 0.003921568859368563\n",
      "b_range : 0.0 0.0039061899296939373\n"
     ]
    }
   ],
   "source": [
    "features = [\"x\",\"y\",\"z\",\"r\",\"g\",\"b\"]\n",
    "for i in range(6): \n",
    "    print(features[i] + \"_range :\", np.min(test_data[:, i]), np.max(test_data[:, i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.33921632170677185, 0.4437101483345032, 0.574175680677096, 0.0, 0.0001845444057835266, 0.0]\n",
      "[0.5573357939720154, 0.6659368872642517, 0.7316666841506958, 0.003921568859368563, 0.003921568859368563, 0.0039061899296939373]\n"
     ]
    }
   ],
   "source": [
    "test_data_min = []\n",
    "test_data_max = []\n",
    "for i in range(6):\n",
    "    test_data_min.append(np.min(test_data[:,i]))\n",
    "    test_data_max.append(np.max(test_data[:,i]))\n",
    "    \n",
    "print(test_data_min)\n",
    "print(test_data_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_range : 0.33921632170677185 0.5573357939720154\n",
      "y_range : 0.4437101483345032 0.6659368872642517\n",
      "z_range : 0.574175680677096 0.7316666841506958\n",
      "r_range : 0.0 0.003921568859368563\n",
      "g_range : 0.0001845444057835266 0.003921568859368563\n",
      "b_range : 0.0 0.0039061899296939373\n"
     ]
    }
   ],
   "source": [
    "features = [\"x\",\"y\",\"z\",\"r\",\"g\",\"b\"]\n",
    "for i in range(6): \n",
    "    print(features[i] + \"_range :\", np.min(test_data[:, i]), np.max(test_data[:, i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Project\\PointNet_Custom_Object_Detection\\model.py:13: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Project\\PointNet_Custom_Object_Detection\\tf_util.py:145: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Project\\PointNet_Custom_Object_Detection\\tf_util.py:21: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Project\\PointNet_Custom_Object_Detection\\tf_util.py:48: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Project\\PointNet_Custom_Object_Detection\\tf_util.py:368: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Tensor(\"fc2/Relu:0\", shape=(1, 128), dtype=float32, device=/device:GPU:0)\n",
      "WARNING:tensorflow:From c:\\Project\\PointNet_Custom_Object_Detection\\tf_util.py:573: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\wangp\\Anaconda3\\envs\\open3d\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from log/model.ckpt\n",
      "Model restored.\n",
      "----\n",
      "0 1\n",
      "(1, 4096)\n",
      "eval mean loss: 0.005112\n",
      "eval accuracy: 0.998291\n",
      "eval avg class acc: 0.998978\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def log_string(out_str):\n",
    "    LOG_FOUT.write(out_str+'\\n')\n",
    "    LOG_FOUT.flush()\n",
    "    print(out_str)\n",
    "\n",
    "\n",
    "def get_learning_rate(batch):\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "                        BASE_LEARNING_RATE,  # Base learning rate.\n",
    "                        batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "                        DECAY_STEP,          # Decay step.\n",
    "                        DECAY_RATE,          # Decay rate.\n",
    "                        staircase=True)\n",
    "    learning_rate = tf.maximum(learning_rate, 0.00001) # CLIP THE LEARNING RATE!!\n",
    "    return learning_rate        \n",
    "\n",
    "def get_bn_decay(batch):\n",
    "    bn_momentum = tf.train.exponential_decay(\n",
    "                      BN_INIT_DECAY,\n",
    "                      batch*BATCH_SIZE,\n",
    "                      BN_DECAY_DECAY_STEP,\n",
    "                      BN_DECAY_DECAY_RATE,\n",
    "                      staircase=True)\n",
    "    bn_decay = tf.minimum(BN_DECAY_CLIP, 1 - bn_momentum)\n",
    "    return bn_decay\n",
    "\n",
    "def evaluate():\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.device('/gpu:'+str(GPU_INDEX)):\n",
    "            pointclouds_pl, labels_pl = placeholder_inputs(BATCH_SIZE, NUM_POINT)\n",
    "            is_training_pl = tf.placeholder(tf.bool, shape=())\n",
    "            \n",
    "            # Note the global_step=batch parameter to minimize. \n",
    "            # That tells the optimizer to helpfully increment the 'batch' parameter for you every time it trains.\n",
    "            batch = tf.Variable(0)\n",
    "            bn_decay = get_bn_decay(batch)\n",
    "            tf.summary.scalar('bn_decay', bn_decay)\n",
    "\n",
    "            # Get model and loss \n",
    "            pred = get_model(pointclouds_pl, is_training_pl, bn_decay=bn_decay)\n",
    "            loss = get_loss(pred, labels_pl)\n",
    "            tf.summary.scalar('loss', loss)\n",
    "            learning_rate = get_learning_rate(batch)\n",
    "\n",
    "            if OPTIMIZER == 'momentum':\n",
    "                optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=MOMENTUM)\n",
    "            elif OPTIMIZER == 'adam':\n",
    "                optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            train_op = optimizer.minimize(loss, global_step=batch)\n",
    "            \n",
    "            # Add ops to save and restore all the variables.\n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "        # Create a session\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        config.allow_soft_placement = True\n",
    "        config.log_device_placement = True\n",
    "        sess = tf.Session(config=config)\n",
    "        merged = tf.summary.merge_all()\n",
    "\n",
    "        \n",
    "        ops = {'pointclouds_pl': pointclouds_pl,\n",
    "       'labels_pl': labels_pl,\n",
    "       'is_training_pl': is_training_pl,\n",
    "       'pred': pred,\n",
    "       'loss': loss,\n",
    "       'train_op': train_op,\n",
    "       'merged': merged,\n",
    "       'step': batch}\n",
    "        MODEL_PATH = \"log/model.ckpt\"\n",
    "        # Restore variables from disk.\n",
    "        saver.restore(sess, MODEL_PATH)\n",
    "        log_string(\"Model restored.\")\n",
    "        \n",
    "        test_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, 'test'))\n",
    "\n",
    "        eval_one_epoch(sess, ops, test_writer)\n",
    "\n",
    "\n",
    "        \n",
    "def eval_one_epoch(sess, ops, test_writer):\n",
    "    \"\"\" ops: dict mapping from string to tf ops \"\"\"\n",
    "    is_training = False\n",
    "    total_correct = 0\n",
    "    total_seen = 0\n",
    "    loss_sum = 0\n",
    "    total_seen_class = [0 for _ in range(NUM_CLASSES)]\n",
    "    total_correct_class = [0 for _ in range(NUM_CLASSES)]\n",
    "    \n",
    "    log_string('----')\n",
    "    \n",
    "    #current_data = np.zeros((1,4096, 6))\n",
    "    current_data  = test_data[0:NUM_POINT,:]\n",
    "    current_label = test_label\n",
    "    \n",
    "    current_data = current_data.reshape(1,4096, 6)\n",
    "    \n",
    "    file_size = current_data.shape[0]\n",
    "    num_batches = file_size // BATCH_SIZE_EVAL \n",
    "    \n",
    "    fout = open('log/'+str(frame_to_test)+'_pred.obj', 'w')\n",
    "    fout_gt = open('log/'+str(frame_to_test)+'_gt.obj', 'w')    \n",
    "    \n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * BATCH_SIZE_EVAL\n",
    "        end_idx = (batch_idx+1) * BATCH_SIZE_EVAL\n",
    "\n",
    "        feed_dict = {ops['pointclouds_pl']: current_data[:, :],\n",
    "                     ops['labels_pl']: current_label[:],\n",
    "                     ops['is_training_pl']: is_training}\n",
    "        summary, step, loss_val, pred_val = sess.run([ops['merged'], ops['step'], ops['loss'], ops['pred']],\n",
    "                                      feed_dict=feed_dict)\n",
    "        \n",
    "        pred_label = np.argmax(pred_val, 2) # BxN\n",
    "        \n",
    "        test_writer.add_summary(summary, step)\n",
    "        pred_val = np.argmax(pred_val, 2)\n",
    "        correct = np.sum(pred_val == current_label[start_idx:end_idx])\n",
    "        total_correct += correct\n",
    "        total_seen += (BATCH_SIZE_EVAL*NUM_POINT)\n",
    "        loss_sum += (loss_val*BATCH_SIZE_EVAL)\n",
    "        class_color = [[0,255,0],[0,0,255]]\n",
    "        print(start_idx, end_idx)\n",
    "        \n",
    "        for i in range(start_idx, end_idx):\n",
    "            print(pred_label.shape)\n",
    "            pred = pred_label[i-start_idx, :]\n",
    "            \n",
    "            pts = current_data[i-start_idx, :, :]\n",
    "            l = current_label[i-start_idx,:]\n",
    "            \n",
    "            for j in range(NUM_POINT):\n",
    "                l = int(current_label[i, j])\n",
    "                total_seen_class[l] += 1\n",
    "                total_correct_class[l] += (pred_val[i-start_idx, j] == l)\n",
    " \n",
    "                color = class_color[pred_val[i-start_idx, j]]\n",
    "                color_gt = class_color[l]\n",
    "  \n",
    "                fout.write('v %f %f %f %d %d %d\\n' % (pts[j,0], pts[j,1], pts[j,2], color[0], color[1], color[2]))\n",
    "                fout_gt.write('v %f %f %f %d %d %d\\n' % (pts[j,0], pts[j,1], pts[j,2], color_gt[0], color_gt[1], color_gt[2]))\n",
    "            \n",
    "    log_string('eval mean loss: %f' % (loss_sum / float(total_seen/NUM_POINT)))\n",
    "    log_string('eval accuracy: %f'% (total_correct / float(total_seen)))\n",
    "    log_string('eval avg class acc: %f' % (np.mean(np.array(total_correct_class)/np.array(total_seen_class,dtype=np.float))))\n",
    "         \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    evaluate()\n",
    "    LOG_FOUT.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-14-7a832d0dda5c>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-14-7a832d0dda5c>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    eval mean loss: 0.297671\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "eval mean loss: 0.297671\n",
    "eval accuracy: 0.840576\n",
    "eval avg class acc: 0.707515"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
